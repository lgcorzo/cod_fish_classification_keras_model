# COD CLASSIFICATION KERAS MODEL

**Case Study:** Classification of fish into three types of cod for the distribution of work in a fish factory. Depending on the size and type of cod, the process to which the fish is subjected is different, so it is necessary to classify into 4 species (**HYSE, SEI, TORSK, LASK**) and measure the length of the fish. They are then sent to hoppers for further processing. See the following image.

![image11](./Media//media/image11.png)

**Problema:** clasificar los peces en 4 especies y medir su longitud.
Las especies son las siguientes: **HYSE,SEI,TORSK,LASK.**

![image3](./Media//media/image3.png)

 **Problem:** Classify fish into 4 species and measure their length. The species are as follows: **HYSE, SEI, TORSK, LASK.**
 The camera captures the fish as they arrive on the conveyor belt, detects the fish on the belt, and from the frames it has, it removes the background and assembles the fish longitudinally, as the camera's field of view is not sufficient to capture the fish in a single frame. This can be seen in the upper images.
 Thus, the classification system receives the following images at a rate of 2 fish per second.


 It seems that convolutional neural networks may be suitable for extracting the characteristics of the species through training of the images, avoiding having to parameterize the characteristics for later classification. With the aim of speeding up the classification process, faster convolutional networks **frcnn** with keras will be tested. (**faster RCNN**) these are region-based convolutional networks.
 The training and classification have been done on a google cloud machine with an nvidia tesla K80 GPU (colab)

 Currently, feature extraction and classification are done in less than 500 msec with a success rate of 95%, requiring 30 minutes of training.

#### **Faster R-CNN**

 It is an RCNN where the selective search algorithm has been replaced with a fast neural network. Specifically, the region proposal network (RPN) was introduced, a regional proposal network.

 How the RPN works:

- On the last layer of an initial CNN, a 3Ã—3 sliding window moves across the feature map to assign it a smaller size (for example, 256-d).
- For each sliding window position, the RPN generates multiple possible regions based on spatial unions of fixed dimensions called anchor boxes.
- Each regional proposal consists of:
  - a score for the presence of the object in that particular region
  - 4 coordinates representing the bounding box of the region.

![image7](./Media//media/image7.png)

 In other words, we look at our region on the last feature map, taking into account the different k anchor boxes around it. For each box, it shows whether it has an object and what the coordinates of the box are. In the image, it is represented as it appears from the position of a sliding window:

 The 2k score represents the probability given by softmax for each k box due to the presence of an object. It is worth noting that, although the RPN processes the coordinates of the bounding boxes, it does not classify the possible objects anyway: it has the sole purpose of identifying regions where there are objects present, therefore, communicating the coordinates with the related boxes. If an anchor box has a score, relative to the presence of an object, above a certain threshold, then that given box will be selected as a possible region.

 Having now our possible regions, we present them directly in the Fast R-CNN. We add a Pooling layer, some fully connected layers, finally a softmax classification layer and a bounding box regressor (regressive bounding box). We can say that Faster R-CNN = RPN + Fast R-CNN.

 ![image4](./Media//media/image4.png)

 The Faster R-CNN thus achieves better speed and accuracy. Although there have been multiple attempts to increase the speed of object recognition, only a few models have been able to surpass this network. In other words, the Faster R-CNN is certainly not the fastest method for object detection, but it presents one of the best performances.

## Application for the case of fish

 A VGG-16 R-CNN has been used and the pre-trained model (nn_base) has been loaded.
 To retrain the system, images from the annotation.txt file were used, which contains a group of images with their bounding box information necessary to use the RPN method to create the proposed bboxes.

 ![image8](./Media//media/image8.png)

 To prepare the data, the **Object_Detection_DataPreprocessing_granit.ipynb** notebook was used, which creates the necessary folders and files for model training with the already trained images and with the defined regions.

 **frcnn_train_vgg.ipynb** trains the model with the images from the train folder and the annotation.txt file. The results of each epoch are saved in the record.csv file. Thus the results of the trainings with the data are as follows. Each epoch consists of training 1000 images generated by data augmentation with rotations, displacements, and mirrors.
 ![image1](./Media//media/image1.png)

![image5](./Media//media/image5.png)

![image2](./Media//media/image2.png)

![image6](./Media//media/image6.png)

 After 68000 batches, the network's precision is 0.944% with an estimation time between 2 and 0.5 seconds. The training time is 6 hours for every 40 epochs. The result is sufficient from epoch 30, thus ensuring that there is no overtraining.

 **frcnn_test_vgg_granit.ipynb** this notebook allows you to see the result in selected images from a gdrive directory with images captured in another period with similar results to the previous ones.

![image10](./Media//media/image10.png)

 Elapsed time = 0.7283480167388916
 \[(\'1_HYSE\', 98.96845817565918), (\'1_HYSE\', 98.17532300949097)\]
 In this image, the system has correctly located two HYSE type fish within the same image.

 **References:**
 [[https://towardsdatascience.com/faster-r-cnn-object-detection-implemented-by-keras-for-custom-data-from-googles-open-images-125f62b9141a]{.ul}](https://towardsdatascience.com/faster-r-cnn-object-detection-implemented-by-keras-for-custom-data-from-googles-open-images-125f62b9141a)
